{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c pytorch torchvision\n",
    "# !pip install skillsnetwork tqdm\n",
    "# !pip install skillsnetwork\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from datetime import datetime\n",
    "#from skillsnetwork import cvstudio\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib.pyplot import imshow\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.cuda\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\avida\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\cifar10\\447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4 (last modified on Sun Sep 10 20:34:36 2023) since it couldn't be found locally at cifar10., or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "cifar_dataset = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the device type is cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"the device type is\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_accuracy(cost, acc) :\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(cost, color=color)\n",
    "    ax1.set_xlabel('Iteration', color=color)\n",
    "    ax1.set_ylabel('Cost', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    color='color:green'\n",
    "    ax2.plot(acc, color=color)\n",
    "    ax2.set_xlabel('Iteration', color=color)\n",
    "    ax2.set_ylabel('Accuracy', color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = cifar_dataset[\"train\"]\n",
    "validation_set = cifar_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['img', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20d8efe6390>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('label: ' + str(train_set[0]['label']))\n",
    "imshow(train_set[0]['img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size=32\n",
    "lr=0.00001\n",
    "momentum=0.9\n",
    "base_lr=0.001\n",
    "max_lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, criterion, optimizer, n_epochs, print_=True) :\n",
    "    loss_list=[]\n",
    "    accuracy_list=[]\n",
    "    correct=0\n",
    "    \n",
    "    n_tests = len(validation_set)\n",
    "    accuracy_best=0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)) :\n",
    "        loss_sublist=[]\n",
    "        \n",
    "        for img, label in train_loader :\n",
    "            img, label = np.array(img).to(device), label.to(device)\n",
    "            model.train()\n",
    "            \n",
    "            z = model(img)\n",
    "            loss = criterion(z, label)\n",
    "            loss_sublist.append(loss.data.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(\"epoch {1} done\".format(epoch))\n",
    "        \n",
    "        scheduler.step()\n",
    "        loss_list.append(np.mean(loss_sublist))\n",
    "        correct = 0\n",
    "        \n",
    "        for img, label in validation_loader :\n",
    "            img_test, label_test = np.array(img_test).to(device), label_test.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            z=model(img)\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            \n",
    "            correct += (yhat == ytest).sum().item()\n",
    "        accuracy = correct / n_tests\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        if accuracy > accuracy_best :\n",
    "            accuracy_best=accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        if print_ :\n",
    "            print(\"The learning rate is\", optimizer.param_groups[0]['lr'])\n",
    "            print(\"The validation cost for epoch \" + str(epoch + 1) + \": \" +str(np.mean(loss_sublist)))\n",
    "            print(\"The validation accuracy for epoch\" + str(epoch + 1) + \": \" + str(accuracy))\n",
    "            \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return accuracy_list, loss_list, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avida\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\avida\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters() :\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want the model to classify 10 different classes, \n",
    "#and the last hidden layer has 512 neurons.\n",
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "convert_tensor = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=means,\n",
    "                                                std=stds)\n",
    "                       ])\n",
    "\n",
    "#train_set = train_set.map(lambda e : {'img' : convert_tensor(e['img']).unsqueeze(0), 'label' : e['label']})\n",
    "#validation_set = validation_set.map(lambda e : {'img' : convert_tensor(e['img']).unsqueeze(0), 'label' : e['label']})\n",
    "# validation_set = validation_set.map(lambda e : {'img' : convert_tensor(e['img']).unsqueeze(0), 'label' : e['label']})\n",
    "\n",
    "#train_list = [(e['img'], e['label']) for e in train_set]\n",
    "#val_list = [(e['img'], e['label']) for e in validation_set]\n",
    "\n",
    "tr_set = pd.DataFrame({\n",
    "    \"img\" : train_set[\"img\"],\n",
    "    \"label\" : train_set[\"label\"]\n",
    "})\n",
    "\n",
    "val_set = pd.DataFrame({\n",
    "    \"img\" : validation_set[\"img\"],\n",
    "    \"label\" : validation_set[\"label\"]\n",
    "})\n",
    "\n",
    "train_loader = DataLoader(dataset=tr_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=1)\n",
    "\n",
    "# next(iter(train_loader))['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img label\n"
     ]
    }
   ],
   "source": [
    "for img, lbl in train_loader :\n",
    "    print(img, lbl) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=5, mode=\"triangular2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]C:\\Users\\avida\\AppData\\Local\\Temp\\ipykernel_15660\\1860368058.py:14: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  img, label = np.array(img).to(device), label.to(device)\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 4 dimensions. The detected shape was (1, 3, 32, 32) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m start_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m acc_list, loss_list, model \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, criterion, optimizer, n_epochs\u001b[38;5;241m=\u001b[39mn_epochs)\n\u001b[0;32m      6\u001b[0m end_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m      7\u001b[0m current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[85], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, validation_loader, criterion, optimizer, n_epochs, print_)\u001b[0m\n\u001b[0;32m     11\u001b[0m loss_sublist\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m train_loader :\n\u001b[1;32m---> 14\u001b[0m     img, label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m     z \u001b[38;5;241m=\u001b[39m model(img)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 4 dimensions. The detected shape was (1, 3, 32, 32) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "start_datetime = datetime.now()\n",
    "start_time = time.time()\n",
    "\n",
    "acc_list, loss_list, model = train_model(model, train_loader, val_loader, criterion, optimizer, n_epochs=n_epochs)\n",
    "\n",
    "end_datetime = datetime.now()\n",
    "current_time = time.time()\n",
    "print(\"time elapsed\", current_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[1;32m----> 2\u001b[0m transform(train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:140\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    138\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'list'>"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 50000)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
